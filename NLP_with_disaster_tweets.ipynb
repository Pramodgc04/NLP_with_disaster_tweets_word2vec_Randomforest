{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLeHVFumRGD2",
        "outputId": "6e40adcb-b2ab-4496-c2c7-316f46706535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92      1866\n",
            "           1       0.77      0.50      0.61       449\n",
            "\n",
            "    accuracy                           0.87      2315\n",
            "   macro avg       0.83      0.73      0.77      2315\n",
            "weighted avg       0.87      0.87      0.86      2315\n",
            "\n",
            "Accuracy (Validation): 0.8734341252699784\n",
            "Cleaned tweets saved to cleaned_tweets.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from gensim.models import Word2Vec\n",
        "import joblib\n",
        "\n",
        "# Download NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load the datasets\n",
        "train_file = 'tweets.csv'\n",
        "data = pd.read_csv(train_file)\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)  # Remove mentions and hashtags\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Remove punctuation and numbers\n",
        "    tokens = nltk.word_tokenize(text)  # Tokenize\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
        "    return tokens\n",
        "\n",
        "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Word2Vec embeddings\n",
        "sentences = data['cleaned_text'].tolist()\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "word2vec_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
        "\n",
        "def get_word2vec_embeddings(tokens):\n",
        "    valid_tokens = [word for word in tokens if word in word2vec_model.wv]\n",
        "    if valid_tokens:\n",
        "        embedding = np.mean([word2vec_model.wv[word] for word in valid_tokens], axis=0)\n",
        "    else:\n",
        "        embedding = np.zeros(100)\n",
        "    return embedding\n",
        "\n",
        "data['embeddings'] = data['cleaned_text'].apply(get_word2vec_embeddings)\n",
        "\n",
        "X = np.vstack(data['embeddings'].values)\n",
        "\n",
        "# Assuming 'target' is the column name for labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data['target'])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model without hyperparameter tuning\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_val_pred = rf.predict(X_val)\n",
        "\n",
        "# Print classification metrics\n",
        "print(\"Classification Report (Validation):\")\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "print(f\"Accuracy (Validation): {accuracy_score(y_val, y_val_pred)}\")\n",
        "\n",
        "# Save the model and necessary objects\n",
        "model_file = 'best_model.pkl'\n",
        "joblib.dump(rf, model_file)\n",
        "\n",
        "word2vec_model_file = 'word2vec_model.pkl'\n",
        "word2vec_model.save(word2vec_model_file)\n",
        "\n",
        "label_encoder_file = 'label_encoder.pkl'\n",
        "joblib.dump(label_encoder, label_encoder_file)\n",
        "\n",
        "# Save the cleaned tweets into a new CSV file\n",
        "cleaned_tweets_file = 'cleaned_tweets.csv'\n",
        "data[['text', 'cleaned_text']].to_csv(cleaned_tweets_file, index=False)\n",
        "\n",
        "print(f\"Cleaned tweets saved to {cleaned_tweets_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DufUfX2S82h",
        "outputId": "76d67d97-1bc6-486a-850a-367886bd35c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:\n",
            "{'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 20}\n",
            "Classification Report after Tuning (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92      1866\n",
            "           1       0.76      0.50      0.60       449\n",
            "\n",
            "    accuracy                           0.87      2315\n",
            "   macro avg       0.83      0.73      0.76      2315\n",
            "weighted avg       0.86      0.87      0.86      2315\n",
            "\n",
            "Accuracy after Tuning (Validation): 0.8730021598272139\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['best_rf_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define a smaller parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['auto', 'sqrt']\n",
        "}\n",
        "\n",
        "# Initialize the random forest classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV with fewer iterations\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=20, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Perform random search\n",
        "rf_random.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best Parameters:\")\n",
        "print(rf_random.best_params_)\n",
        "\n",
        "# Use the best estimator to predict on validation set\n",
        "best_rf = rf_random.best_estimator_\n",
        "y_val_pred_tuned = best_rf.predict(X_val)\n",
        "\n",
        "# Print classification metrics after tuning\n",
        "print(\"Classification Report after Tuning (Validation):\")\n",
        "print(classification_report(y_val, y_val_pred_tuned))\n",
        "print(f\"Accuracy after Tuning (Validation): {accuracy_score(y_val, y_val_pred_tuned)}\")\n",
        "\n",
        "# Save the best model\n",
        "best_model_file = 'best_rf_model.pkl'\n",
        "joblib.dump(best_rf, best_model_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import joblib\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Function to get Word2Vec embeddings\n",
        "def get_word2vec_embeddings(tokens):\n",
        "    if isinstance(tokens, list):\n",
        "        valid_tokens = [word for word in tokens if word in word2vec_model.wv]\n",
        "        if valid_tokens:\n",
        "            embedding = np.mean([word2vec_model.wv[word] for word in valid_tokens], axis=0)\n",
        "        else:\n",
        "            embedding = np.zeros(100)\n",
        "    else:\n",
        "        embedding = np.zeros(100)\n",
        "    return embedding\n",
        "\n",
        "# Load the saved Word2Vec model\n",
        "word2vec_model = Word2Vec.load('word2vec_model.pkl')\n",
        "\n",
        "# Load the saved Random Forest model\n",
        "best_rf = joblib.load('best_rf_model.pkl')\n",
        "\n",
        "# Load the saved label encoder\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "\n",
        "# Load the new dataset\n",
        "test_file = 'test.csv'  # Replace with your actual test file\n",
        "test_data = pd.read_csv(test_file)\n",
        "\n",
        "# Ensure that 'cleaned_text' column exists and is preprocessed\n",
        "if 'cleaned_text' not in test_data.columns:\n",
        "    raise ValueError(\"The 'cleaned_text' column is missing from the dataset.\")\n",
        "\n",
        "# Create embeddings for the preprocessed text\n",
        "test_data['embeddings'] = test_data['cleaned_text'].apply(get_word2vec_embeddings)\n",
        "X_test = np.vstack(test_data['embeddings'].values)\n",
        "\n",
        "# Encode the target labels\n",
        "y_test = label_encoder.transform(test_data['target'])\n",
        "\n",
        "# Predict the target using the loaded model\n",
        "y_test_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Print classification metrics\n",
        "print(\"Classification Report (Test):\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "print(f\"Accuracy (Test): {accuracy_score(y_test, y_test_pred)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGZ_C9qX6EFY",
        "outputId": "c0b64493-337e-4ae4-95f5-120f110a9059"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      1.00      0.73      4401\n",
            "           1       0.00      0.00      0.00      3212\n",
            "\n",
            "    accuracy                           0.58      7613\n",
            "   macro avg       0.29      0.50      0.37      7613\n",
            "weighted avg       0.33      0.58      0.42      7613\n",
            "\n",
            "Accuracy (Test): 0.5780901090240378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrU9FvTDb6N1",
        "outputId": "e61d9c81-9b9a-476e-8f4d-cc8b9893341a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for the test sentence 'This is a test tweet to check the model prediction.': [0]\n",
            "Prediction for the test sentence 'Another example sentence for prediction testing.': [0]\n",
            "Prediction for the test sentence 'The rain in Spain falls mainly on the plain.': [0]\n",
            "Prediction for the test sentence 'the Tsunami killed thousands.': [1]\n",
            "Prediction for the test sentence 'the thunderstorm hit our neighbour's house': [1]\n",
            "Prediction for the test sentence 'a house flew in the air due to whirlwind this afternoon': [1]\n",
            "Prediction for the test sentence 'The first train to cross the world's highest railway bridge-the Chenab bridge in india': [0]\n",
            "Prediction for the test sentence 'Since it's yoga day, it's the perfect image to signify that our infrastructure is stretching itself as far towardsthe skies as possible ': [0]\n",
            "Prediction for the test sentence 'Massive fire has broken out near the oak street bridge in richmond of british colombia': [1]\n",
            "Prediction for the test sentence 'Firefighters are working tirelessly to contain a fire at a local business. Support them by avoiding the area.': [1]\n",
            "Prediction for the test sentence 'Severe thunderstorm warning in effect. Stay indoors and avoid travel if possible.Recovery and rebuilding after the tsunami will require a global effort. Let's stand together.': [1]\n",
            "Prediction for the test sentence 'Tornado spotted! Take cover immediately and follow emergency instructions.': [1]\n",
            "Prediction for the test sentence 'Hurricane making landfall with strong winds and heavy rain. Evacuate if advised and stay indoors.': [1]\n",
            "Prediction for the test sentence 'A strong earthquake has just struck. Check on your loved ones and follow safety protocols.': [1]\n",
            "Prediction for the test sentence 'Smoky skies and blazing fires. Our thoughts are with everyone in the path of the wildfires.': [1]\n",
            "Prediction for the test sentence 'Inundated streets and rising waters. Thoughts are with everyone affected by the floods.': [0]\n",
            "Prediction for the test sentence 'Downed trees and power lines reported due to the storm. Stay away from any fallen wires.': [1]\n",
            "Prediction for the test sentence 'Had an amazing dinner with friends last night.': [0]\n",
            "Prediction for the test sentence 'Just finished a great workout at the gym!': [0]\n",
            "Prediction for the test sentence 'Had an amazing dinner with friends last night.': [0]\n",
            "Prediction for the test sentence 'Enjoying a relaxing day at the beach.': [0]\n",
            "Prediction for the test sentence 'Watching a new movie on Netflix tonight.': [0]\n",
            "Prediction for the test sentence 'Just adopted a new puppy, can't wait to bring him home!': [0]\n",
            "Prediction for the test sentence 'Reading a fantastic book on my day off.': [0]\n",
            "Prediction for the test sentence 'Exploring the city and finding new coffee shops.': [0]\n",
            "Prediction for the test sentence 'Attending a concert this weekend, so excited!': [0]\n",
            "Prediction for the test sentence 'Spent the day gardening, love being outdoors.': [0]\n",
            "Prediction for the test sentence 'Catching up on some much-needed sleep today.': [0]\n"
          ]
        }
      ],
      "source": [
        "# Test the model with a single sentence\n",
        "def test_single_sentence(sentence):\n",
        "    tokens = preprocess_text(sentence)\n",
        "    embedding = get_word2vec_embeddings(tokens)\n",
        "    embedding = embedding.reshape(1, -1)\n",
        "    prediction = best_rf.predict(embedding)\n",
        "    return prediction\n",
        "\n",
        "# Example test with the provided test data\n",
        "test_sentences = [\n",
        "    \"This is a test tweet to check the model prediction.\",\n",
        "    \"Another example sentence for prediction testing.\",\n",
        "    \"The rain in Spain falls mainly on the plain.\",\n",
        "    \"the Tsunami killed thousands.\",\n",
        "    \"the thunderstorm hit our neighbour's house\",\n",
        "    \"a house flew in the air due to whirlwind this afternoon\",\n",
        "    \"The first train to cross the world's highest railway bridge-the Chenab bridge in india\",\n",
        "    \"Since it's yoga day, it's the perfect image to signify that our infrastructure is stretching itself as far towardsthe skies as possible \",\n",
        "    \"Massive fire has broken out near the oak street bridge in richmond of british colombia\",\n",
        "    \"Firefighters are working tirelessly to contain a fire at a local business. Support them by avoiding the area.\",\n",
        "    \"Severe thunderstorm warning in effect. Stay indoors and avoid travel if possible.\"\n",
        "    \"Recovery and rebuilding after the tsunami will require a global effort. Let's stand together.\",\n",
        "    \"Tornado spotted! Take cover immediately and follow emergency instructions.\",\n",
        "    \"Hurricane making landfall with strong winds and heavy rain. Evacuate if advised and stay indoors.\",\n",
        "    \"A strong earthquake has just struck. Check on your loved ones and follow safety protocols.\",\n",
        "    \"Smoky skies and blazing fires. Our thoughts are with everyone in the path of the wildfires.\",\n",
        "    \"Inundated streets and rising waters. Thoughts are with everyone affected by the floods.\",\n",
        "    \"Downed trees and power lines reported due to the storm. Stay away from any fallen wires.\",\n",
        "    \"Had an amazing dinner with friends last night.\",\n",
        "    \"Just finished a great workout at the gym!\",\n",
        "    \"Had an amazing dinner with friends last night.\",\n",
        "    \"Enjoying a relaxing day at the beach.\",\n",
        "    \"Watching a new movie on Netflix tonight.\",\n",
        "    \"Just adopted a new puppy, can't wait to bring him home!\",\n",
        "    \"Reading a fantastic book on my day off.\",\n",
        "    \"Exploring the city and finding new coffee shops.\",\n",
        "    \"Attending a concert this weekend, so excited!\",\n",
        "    \"Spent the day gardening, love being outdoors.\",\n",
        "    \"Catching up on some much-needed sleep today.\"\n",
        "\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    prediction = test_single_sentence(sentence)\n",
        "    print(f\"Prediction for the test sentence '{sentence}': {prediction}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!pip install joblib\n",
        "!pip install numpy\n",
        "!pip install nltk\n",
        "!pip install gensim\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2cb8hPWuERvf",
        "outputId": "ce7e4907-1d02-4b33-a148-f32fc74edf19"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.36.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.4.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.6)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import joblib\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "import requests\n",
        "import base64\n",
        "from datetime import datetime, timedelta\n",
        "import tweepy  # Import Tweepy for Twitter API interaction\n",
        "\n",
        "# Download NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load the trained models and other necessary objects\n",
        "model = joblib.load('best_rf_model.pkl')\n",
        "word2vec_model = Word2Vec.load('word2vec_model.pkl')\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "\n",
        "# Load and encode the background image\n",
        "def get_base64_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        encoded_string = base64.b64encode(image_file.read()).decode()\n",
        "    return encoded_string\n",
        "\n",
        "bg_image = get_base64_image('natural.png')  # Replace with actual image path\n",
        "\n",
        "# Custom CSS\n",
        "st.markdown(\n",
        "    f\"\"\"\n",
        "    <style>\n",
        "    .stApp {{\n",
        "        background-image: url(\"data:image/jpeg;base64,{bg_image}\");\n",
        "        background-size: cover;\n",
        "    }}\n",
        "\n",
        "    .stButton>button {{\n",
        "        background-color: #071952;\n",
        "        color: white;\n",
        "        padding: 10px 24px;\n",
        "        font-size: 16px;\n",
        "        border: none;\n",
        "        border-radius: 5px;\n",
        "    }}\n",
        "    .stTextInput>div>div>input {{\n",
        "        padding: 5px;\n",
        "        font-size: 16px;\n",
        "    }}\n",
        "    .disaster {{\n",
        "        color: white;\n",
        "        background-color: red;\n",
        "        padding: 10px;\n",
        "        border-radius: 5px;\n",
        "    }}\n",
        "    .non-disaster {{\n",
        "        color: white;\n",
        "        background-color: green;\n",
        "        padding: 10px;\n",
        "        border-radius: 5px;\n",
        "    }}\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "# Preprocess the input text\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r'\\@\\w+|\\#','', text)  # Remove mentions and hashtags\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Remove punctuation and numbers\n",
        "    tokens = nltk.word_tokenize(text)  # Tokenize\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
        "    return tokens\n",
        "\n",
        "def get_word2vec_embeddings(tokens):\n",
        "    valid_tokens = [word for word in tokens if word in word2vec_model.wv]\n",
        "    if valid_tokens:\n",
        "        embedding = np.mean([word2vec_model.wv[word] for word in valid_tokens], axis=0)\n",
        "    else:\n",
        "        embedding = np.zeros(100)\n",
        "    return embedding\n",
        "\n",
        "# Function to fetch tweet text from URL\n",
        "def fetch_tweet_text_from_url(url, bearer_token):\n",
        "    tweet_id = url.split('/')[-1]\n",
        "    tweet_url = f\"https://x.com/mrpxssy/status/{tweet_id}\"\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {bearer_token}',\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(tweet_url, headers=headers)\n",
        "        response.raise_for_status()  # Raise error for non-200 status codes\n",
        "        tweet = response.json()\n",
        "        return tweet['data']['text']\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        st.error(f\"HTTP error occurred: {http_err}\")\n",
        "    except Exception as err:\n",
        "        st.error(f\"Error fetching content from URL: {err}\")\n",
        "    return None\n",
        "\n",
        "# Function to fetch recent disaster-related tweets\n",
        "def fetch_recent_disaster_tweets():\n",
        "    auth = tweepy.AppAuthHandler('KAMf7yXekENs9em5Z8pkltIMB', 'K0EVVBACVJO3moAe5ILNwwxDDLMPM1UN2slRQJKCCHXFnyH7Hg')  # Replace with your actual consumer key and secret\n",
        "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
        "\n",
        "    tweets = []\n",
        "    disaster_keywords = ['disaster', 'emergency', 'earthquake', 'flood', 'wildfire']  # Adjust as needed\n",
        "\n",
        "    for keyword in disaster_keywords:\n",
        "        for tweet in tweepy.Cursor(api.search, q=keyword, lang=\"en\", tweet_mode=\"extended\").items(10):\n",
        "            if (datetime.now() - tweet.created_at) < timedelta(hours=4):\n",
        "                tweets.append(tweet.full_text)\n",
        "    return tweets\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.markdown(\"<div class='main'>\", unsafe_allow_html=True)\n",
        "    st.title(\"Disaster Tweet Classifier\")\n",
        "    st.write(\"Enter a tweet or a URL leading to a tweet to predict if it's a disaster tweet or not.\")\n",
        "\n",
        "    tweet_or_url = st.text_area(\"Enter Tweet Text or URL\")\n",
        "\n",
        "    if st.button(\"Fetch & Classify\"):\n",
        "        if tweet_or_url.startswith(\"http\"):\n",
        "            tweet_text = fetch_tweet_text_from_url(tweet_or_url, 'AAAAAAAAAAAAAAAAAAAAAIzPugEAAAAA8kO%2FbxNWLXP6H8xJI80%2BNIWbMac%3DkKB4ROYsPOEXMltPFOiXMriagrlTcEno34VLzEKs8yfMQSHsVq')  # Replace with your actual bearer token\n",
        "            if tweet_text:\n",
        "                st.write(f\"Tweet: {tweet_text}\")\n",
        "                cleaned_text = preprocess_text(tweet_text)\n",
        "            else:\n",
        "                st.error(\"Failed to fetch content from URL.\")\n",
        "                return\n",
        "        else:\n",
        "            tweet_text = tweet_or_url\n",
        "            st.write(f\"Tweet: {tweet_text}\")\n",
        "            cleaned_text = preprocess_text(tweet_or_url)\n",
        "\n",
        "        embedding = get_word2vec_embeddings(cleaned_text).reshape(1, -1)\n",
        "        prediction = model.predict(embedding)\n",
        "        prediction_l = label_encoder.inverse_transform(prediction)[0]\n",
        "\n",
        "        if prediction_l == 1:\n",
        "            st.markdown(f\"<h3 class='disaster'>Prediction: Disaster Tweet</h3>\", unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.markdown(f\"<h3  class='non-disaster'>Prediction: Not a Disaster Tweet</h3>\", unsafe_allow_html=True)\n",
        "\n",
        "    if st.button(\"Fetch Recent Disaster Tweets\"):\n",
        "        st.subheader(\"Recent Disaster Tweets (last 4 hours)\")\n",
        "        recent_tweets = fetch_recent_disaster_tweets()\n",
        "        for tweet in recent_tweets:\n",
        "            st.write(tweet)\n",
        "            st.write(\"---\")\n",
        "\n",
        "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUCS2AOERQ-N",
        "outputId": "d11c4687-35f7-4dec-dd39-54b81b22def6"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Set up your ngrok authentication token\n",
        "ngrok.set_auth_token('2ix585YsTPlv9xecr2xDI4bvL9W_7w44VF52rSSHdtB4zTKTQ')  # Replace with your actual ngrok auth token\n",
        "\n",
        "# Kill any existing ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Run the Streamlit app\n",
        "streamlit_proc = subprocess.Popen(['streamlit', 'run', 'app.py', '--server.port', '8501'])\n",
        "\n",
        "# Wait a few seconds for the app to start\n",
        "time.sleep(20)  # Increase the sleep time if necessary\n",
        "\n",
        "# Create a tunnel to the Streamlit port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print('Public URL:', public_url)\n",
        "\n",
        "# Keep the tunnel open\n",
        "try:\n",
        "    streamlit_proc.communicate()\n",
        "except KeyboardInterrupt:\n",
        "    streamlit_proc.terminate()\n",
        "    ngrok.kill()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL2cfN1jGuKh",
        "outputId": "3980985f-a4d3-4b0c-8bbb-052e198f43fb"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://fe6e-34-121-49-203.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g7oighQ8dJ4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}